{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims): #layer_dims : list of dimensions of each layers in our NN\n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.00496714 -0.00138264  0.00647689]\n",
      " [ 0.0152303  -0.00234153 -0.00234137]\n",
      " [ 0.01579213  0.00767435 -0.00469474]\n",
      " [ 0.0054256  -0.00463418 -0.0046573 ]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.00241962 -0.0191328  -0.01724918 -0.00562288]\n",
      " [-0.01012831  0.00314247 -0.00908024 -0.01412304]\n",
      " [ 0.01465649 -0.00225776  0.00067528 -0.01424748]\n",
      " [-0.00544383  0.00110923 -0.01150994  0.00375698]\n",
      " [-0.00600639 -0.00291694 -0.00601707  0.01852278]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "params = initialize_parameters([3,4,5])\n",
    "print('W1 = ' + str(params['W1']))\n",
    "print('b1 = ' + str(params['b1']))\n",
    "print('W2 = ' + str(params['W2']))\n",
    "print('b2 = ' + str(params['b2']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper fuction_1\n",
    "def linear_forward(A,W,b):\n",
    "    '''\n",
    "    A : activations from previous layer --> shape -->(size of prev. layer , number of examples)\n",
    "    W : weights matrix , numpy array of shape --> (size of current layer , size of prev. layer)\n",
    "    b : bias vector , numpy array of shape --> (size of current layer , 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z : the input to the activation function\n",
    "    cache : a python tuple containing 'A', 'W', 'b', stored for efficient back propagation.\n",
    "\n",
    "    '''\n",
    "    Z = np.dot(W,A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function_2\n",
    "def sigmoid(z):\n",
    "    A = 1/(1+np.exp(-z))\n",
    "    return A,z\n",
    "\n",
    "def tanh(z):\n",
    "    A = np.tanh(z)\n",
    "    return A,z\n",
    "\n",
    "def relu(z):\n",
    "    A = np.maximum(0,z)\n",
    "    return A,z\n",
    "\n",
    "def leaky_relu(z):\n",
    "    A = np.maximum(0.01*z , z)\n",
    "    return A,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function_3\n",
    "def linear_activation_forward(A_prev , W , b, activation):\n",
    "    '''\n",
    "    A : activations from previous layer --> shape -->(size of prev. layer , number of examples)\n",
    "    W : weights matrix , numpy array of shape --> (size of current layer , size of prev. layer)\n",
    "    b : bias vector , numpy array of shape --> (size of current layer , 1)\n",
    "    activation: the activation to be used, passed as string.\n",
    "    \n",
    "    Returns:\n",
    "    A : the output of the activation function also called post activation value\n",
    "    cache : a python tuple conatining 'linear_cache', 'activation_cache'.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    elif activation == 'relu':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(z)\n",
    "        \n",
    "    elif activation == 'tanh':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A , activation_cache = tanh(z)\n",
    "        \n",
    "    elif activation == 'leaky_relu':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = leaky_relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    '''\n",
    "    Implementing forward prop. for the [LINEAR->RELU]*(L-1) -> LINEAR->SIGMOID computation,\n",
    "    can be changed according to need.\n",
    "    \n",
    "    X : data, numpy array of shape -->(input_size, number of examples)\n",
    "    parameters : output of initialize_parameters function.\n",
    "    \n",
    "    Returns:\n",
    "    AL : last post-activation value\n",
    "    caches : list of caches conataining:\n",
    "             every cache pf linear_activation_forward()\n",
    "    '''\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2   #gives the number of layers in neural network.\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev , parameters['W' + str(l)], parameters['b' + str(l)] , activation = 'relu')\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)] , parameters['b'+ str(L)], activation = 'sigmoid')\n",
    "    cahes.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing cost function (cross entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    '''\n",
    "    Al = probability vector corresponding to label predictions -->(1 , number of examples)\n",
    "    Y = true 'label' vector -->(1, number of examples)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    cost = -1/m * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1-Y , np.log(1 - AL)))\n",
    "    \n",
    "    cost = np.squeeze(cost)      #converts [[15]] to 15 \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ , cache):\n",
    "    '''\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "   \n",
    "   '''\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ , cache[0].T)/m\n",
    "    db = (np.sum(dZ, axis = 1, keepdims= True))/m\n",
    "    dA_prev = np.dot(cache[1], dZ)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backwards(dA ,cache):\n",
    "    Z = cache\n",
    "    dZ = dA*(sigmoid(Z)*(1-sigmoid(Z)))\n",
    "    return dZ\n",
    "\n",
    "def relu_backwards(dA,cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA,copy = True)\n",
    "    dZ[Z<=0] = 0\n",
    "    return dZ\n",
    "\n",
    "def tanh_backwards(dA , cache):\n",
    "    Z = cache\n",
    "    dZ = (1-tanh(z))**2\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    '''\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    '''\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == 'relu':\n",
    "        dZ = relu_backwards(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "    \n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = sigmoid_backwards(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    elif activation == 'tanh':\n",
    "        dZ = tanh_backwards(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL , Y, caches):\n",
    "    '''\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1-Y,1 -AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads['dA'+str(L-1)] , grads['dW'+str(L)], grads['db'+str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid')\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = cache[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads['dA'+str(l+1)], current_cache, 'relu')\n",
    "        grads['dA'+str(l)] = dA_prev_temp\n",
    "        grads['dW' + str(l+1)] = dW_temp\n",
    "        grads['db'+str(l+1)] = db_temp\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    '''\n",
    "        Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \n",
    "    '''\n",
    "    L = len(parameters)\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters['W'+str(l+1)] = parameters['W'+str(l+1)] - learning_rate*grads['dW'+str(l+1)]\n",
    "        parameters['b'+str(l+1)] = parameters['b'+str(l+1)] -learning_rate*grads['db'+str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model eg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations=3000, print_cost=False):\n",
    "    '''\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "\n",
    "\n",
    "    '''\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    for i in range(0,num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        cost = compute_cost(AL,Y)\n",
    "        grads = L_model_backward(AL, Y , caches)\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        \n",
    "    # printing cost after every 100 examples\n",
    "    if print_cost and i%100 ==0:\n",
    "        print('Cost after every iteration %i: %f' %(i, cost))\n",
    "    if print_cost and i%100 == 0:\n",
    "        costs.append(cost)\n",
    "    \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "   \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
